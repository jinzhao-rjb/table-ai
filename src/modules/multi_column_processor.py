#!/usr/bin/env python3
"""
AIå‡½æ•°ç”Ÿæˆä¸è°ƒç”¨çš„æ ¸å¿ƒå…¥å£
è´Ÿè´£åŠ¨æ€æç¤ºè¯ç”Ÿæˆã€AIæœåŠ¡è°ƒç”¨ã€å‡½æ•°è§£æä¸åº”ç”¨
"""

import os
import sys
import pandas as pd
import logging
import json
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.modules.ai_service import get_ai_service
from src.modules.prompt_generator import PromptGenerator
from src.modules.vectorized_function_converter import VectorizedFunctionConverter
from src.utils.dual_redis_db import DualRedisDB

logger = logging.getLogger("MultiColumnProcessor")

class MultiColumnProcessor:
    """
    AIå‡½æ•°ç”Ÿæˆä¸è°ƒç”¨çš„æ ¸å¿ƒå¤„ç†å™¨
    è´Ÿè´£æ ¹æ®éœ€æ±‚å’Œæ•°æ®ä¸Šä¸‹æ–‡ç”ŸæˆAIå‡½æ•°ï¼Œå¹¶åº”ç”¨äºæ•°æ®å¤„ç†
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–å¤šåˆ—å¤„ç†å™¨
        """
        self.ai_service = None
        self.prompt_generator = PromptGenerator()
        self.vectorized_converter = VectorizedFunctionConverter()
        self.logger = logger
        # ä½¿ç”¨ DualRedisDB æ›¿ä»£åŸæ¥çš„ QwenDB
        self.dual_redis = DualRedisDB()
        self.qwen_db = self.dual_redis  # ä¿æŒå‘åå…¼å®¹
        # è®°å½•æœ€åä¸€æ¬¡å¤±è´¥çš„ä»£ç 
        self.last_failed_code = []
    
    def set_ai_service(self, ai_service):
        """
        è®¾ç½®AIæœåŠ¡å®ä¾‹
        
        Args:
            ai_service: AIæœåŠ¡å®ä¾‹
        """
        self.ai_service = ai_service
    
    def _get_ai_service(self):
        """
        è·å–AIæœåŠ¡å®ä¾‹
        
        Returns:
            AIæœåŠ¡å®ä¾‹
        """
        if not self.ai_service:
            self.ai_service = get_ai_service()
        return self.ai_service
    
    def _load_excel_data(self, file_path: str) -> pd.DataFrame:
        """
        åŠ è½½Excelæ•°æ®
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½åçš„DataFrame
        """
        try:
            df = pd.read_excel(file_path)
            self.logger.info(f"æˆåŠŸåŠ è½½Excelæ–‡ä»¶: {file_path}")
            self.logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            self.logger.info(f"æ•°æ®åˆ—: {list(df.columns)}")
            return df
        except Exception as e:
            self.logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _generate_data_context(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®ä¸Šä¸‹æ–‡
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            æ•°æ®ä¸Šä¸‹æ–‡å­—å…¸
        """
        # è¯†åˆ«åˆ—ç±»å‹
        numeric_columns = list(df.select_dtypes(include=['number']).columns)
        date_columns = list(df.select_dtypes(include=['datetime64']).columns)
        categorical_columns = list(df.select_dtypes(include=['object']).columns)
        
        # è·å–æ•°æ®ç±»å‹ä¿¡æ¯
        data_types = {col: str(df[col].dtype) for col in df.columns}
        
        # ç”Ÿæˆæ•°æ®ä¸Šä¸‹æ–‡ - å‡å°‘sample_dataçš„å¹²æ‰°ï¼Œåªä¿ç•™å¿…è¦ä¿¡æ¯
        data_context = {
            "columns": list(df.columns),
            "data_types": data_types,
            "data_shape": df.shape,
            "numeric_columns": numeric_columns,
            "date_columns": date_columns,
            "categorical_columns": categorical_columns,
            # åªä¿ç•™å°‘é‡æ ·æœ¬æ•°æ®ï¼Œå‡å°‘å¹²æ‰°
            "sample_data": [],
            "data_info": {
                "total_rows": len(df),
                "total_columns": len(df.columns),
                "numeric_count": len(numeric_columns),
                "date_count": len(date_columns),
                "categorical_count": len(categorical_columns)
            }
        }
        
        self.logger.info(f"ç”Ÿæˆæ•°æ®ä¸Šä¸‹æ–‡: {data_context}")
        return data_context
    
    def generate_multi_column_functions(self, requirement: str, data_context: Any, last_error: str = "", iteration: int = 1, max_iterations: int = 3) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå¤šåˆ—å¤„ç†å‡½æ•°
        
        Args:
            requirement: ç”¨æˆ·éœ€æ±‚
            data_context: æ•°æ®ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æ˜¯DataFrameæˆ–ä¸Šä¸‹æ–‡å­—å…¸
            last_error: ä¸Šä¸€æ¬¡çš„é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºä¼˜åŒ–æç¤ºè¯
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Returns:
            ç”Ÿæˆçš„å‡½æ•°åˆ—è¡¨
        """
        try:
            # å¦‚æœä¼ å…¥çš„æ˜¯DataFrameï¼Œè½¬æ¢ä¸ºæ•°æ®ä¸Šä¸‹æ–‡
            if isinstance(data_context, pd.DataFrame):
                data_context = self._generate_data_context(data_context)
            
            # è·å–AIæœåŠ¡
            ai_service = self._get_ai_service()
            
            # ä» Redis è·å–å†å²é”™è¯¯è®°å¿†
            historical_error = ""
            if ai_service.qwen_learning:
                historical_error = ai_service.qwen_learning.get_error_memory(requirement)
            
            # ç”Ÿæˆæç¤ºè¯
            prompt = self.prompt_generator.generate_prompt(
                requirement=requirement,
                data_context=data_context
            )
            
            # åˆå¹¶æ‰€æœ‰é”™è¯¯ä¿¡æ¯
            all_errors = []
            if last_error:
                all_errors.append(f"ä¸Šä¸€æ¬¡è¿­ä»£é”™è¯¯: {last_error}")
            if historical_error:
                all_errors.append(f"å†å²é”™è¯¯: {historical_error}")
            
            # å¦‚æœæœ‰é”™è¯¯ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
            if all_errors:
                prompt += "\n\n" + "\n\n".join(all_errors)
                prompt += "\n\nè¯·ç¡®ä¿ç”Ÿæˆçš„å‡½æ•°èƒ½å¤Ÿï¼š"
                prompt += "\n1. ç”Ÿæˆæ–°çš„è®¡ç®—åˆ—ï¼Œä¾‹å¦‚ï¼šæ¯›åˆ©ç‡ = (é”€å”®é¢ - æˆæœ¬) / é”€å”®é¢ * 100"
                prompt += "\n2. ä½¿ç”¨æœ‰æ„ä¹‰çš„åˆ—åï¼Œä¾‹å¦‚ï¼šæ¯›åˆ©ç‡ã€å¹´é”€å”®é¢æ€»å’Œã€åŒæ¯”å¢é•¿ç­‰"
                prompt += "\n3. ç›´æ¥æ‰§è¡Œï¼Œæ— éœ€é¢å¤–ä¿®æ”¹"
                prompt += "\n4. åŒ…å«å®Œæ•´çš„å¼‚å¸¸å¤„ç†"
                prompt += "\n5. ç‰¹åˆ«æ³¨æ„é¿å…ä¸Šè¿°å†å²é”™è¯¯å’Œä¸Šä¸€æ¬¡è¿­ä»£é”™è¯¯"
            
            # å½“æ¥è¿‘æœ€å¤§è¿­ä»£æ¬¡æ•°æ—¶ï¼Œä»Redisè·å–ç±»ä¼¼æˆåŠŸæ¡ˆä¾‹ä½œä¸ºå‚è€ƒ
            if iteration >= max_iterations - 1:
                ai_service = self._get_ai_service()
                if ai_service.qwen_learning:
                    # ä»Redisè·å–ç±»ä¼¼æˆåŠŸæ¡ˆä¾‹
                    similar_cases = self.dual_redis.db_conn.get(f"success_cases:{requirement[:50]}")
                    if similar_cases:
                        try:
                            similar_cases = json.loads(similar_cases)
                            if similar_cases:
                                prompt += "\n\nå‚è€ƒæˆåŠŸæ¡ˆä¾‹ï¼š"
                                for i, case in enumerate(similar_cases[:3]):
                                    prompt += f"\næ¡ˆä¾‹ {i+1}: {case[:200]}..."
                        except json.JSONDecodeError:
                            self.logger.warning("è§£æç±»ä¼¼æˆåŠŸæ¡ˆä¾‹å¤±è´¥")
            
            # è·å–AIæœåŠ¡
            ai_service = self._get_ai_service()
            
            # è°ƒç”¨AIç”Ÿæˆå‡½æ•°
            functions = ai_service.generate_functions(prompt, data_context)
            
            # éªŒè¯ç”Ÿæˆçš„å‡½æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚
            valid_functions = []
            for func in functions:
                func_implementation = func.get('implementation', '')
                # æ£€æŸ¥å‡½æ•°æ˜¯å¦åŒ…å«ç”Ÿæˆæ–°åˆ—çš„ä»£ç 
                if 'df[' in func_implementation or 'df["' in func_implementation:
                    valid_functions.append(func)
                else:
                    self.logger.warning(f"å‡½æ•° {func.get('name', 'unknown')} æœªç”Ÿæˆæ–°åˆ—ï¼Œè·³è¿‡")
            
            if valid_functions:
                self.logger.info(f"æˆåŠŸç”Ÿæˆ {len(valid_functions)} ä¸ªæœ‰æ•ˆçš„å‡½æ•°")
                return valid_functions
            else:
                self.logger.warning("æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‡½æ•°ï¼Œè¿”å›åŸå§‹å‡½æ•°åˆ—è¡¨")
                return functions
        except json.JSONDecodeError as e:
            self.logger.error(f"ç”Ÿæˆå‡½æ•°å¤±è´¥: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
            # å¢å¼ºå®¹é”™æ€§ï¼šå°è¯•ä»AIå“åº”ä¸­ç›´æ¥æå–Pythonä»£ç 
            import re
            ai_service = self._get_ai_service()
            # ä»AIå“åº”ä¸­å°è¯•æå–Pythonä»£ç 
            last_response = getattr(ai_service, 'last_response', '')
            if last_response:
                extracted_functions = self._extract_functions_from_response(last_response, data_context)
                if extracted_functions:
                    self.logger.info(f"ä»AIå“åº”ä¸­æå–åˆ° {len(extracted_functions)} ä¸ªå‡½æ•°")
                    return extracted_functions
            return []
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå‡½æ•°å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return []

    def _extract_functions_from_response(self, response: str, data_context: Dict) -> List[Dict[str, Any]]:
        """
        ä»AIå“åº”ä¸­æå–å‡½æ•°å®šä¹‰
            
        Args:
            response: AIçš„åŸå§‹å“åº”
            data_context: æ•°æ®ä¸Šä¸‹æ–‡
            
        Returns:
            æå–åˆ°çš„å‡½æ•°åˆ—è¡¨
        """
        import re
        import json
            
        # é¦–å…ˆå°è¯•ä»å“åº”ä¸­æå–JSONéƒ¨åˆ†
        json_pattern = r'\[\s*\{.*?\}\s*\]'  # åŒ¹é…JSONæ•°ç»„
        json_match = re.search(json_pattern, response, re.DOTALL)
            
        if json_match:
            try:
                json_str = json_match.group(0)
                # å°è¯•ä¿®å¤JSONæ ¼å¼
                json_str = self._fix_json_format(json_str)
                functions = json.loads(json_str)
                if isinstance(functions, list):
                    return functions
            except json.JSONDecodeError:
                pass
            
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥æå–Pythonä»£ç 
        # åŒ¹é…å‡½æ•°å®šä¹‰æ¨¡å¼
        code_pattern = r'def\s+\w+\s*\([^)]*\):[^{]+?return[^}]*df[^}]*'
        code_matches = re.findall(code_pattern, response, re.DOTALL)
            
        # æå–Pythonä»£ç å—
        code_block_pattern = r'```python\s*(.*?)\s*```'
        code_blocks = re.findall(code_block_pattern, response, re.DOTALL)
            
        if code_blocks:
            functions = []
            for i, code_block in enumerate(code_blocks):
                # å°è¯•ä»ä»£ç å—ä¸­æå–å‡½æ•°å
                func_name_match = re.search(r'def\s+(\w+)', code_block)
                func_name = func_name_match.group(1) if func_name_match else f'extracted_func_{i}'
                    
                functions.append({
                    'name': func_name,
                    'description': f'ä»AIå“åº”ä¸­æå–çš„å‡½æ•°: {func_name}',
                    'implementation': code_block,
                    'required_columns': []
                })
            return functions
            
        # å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œå°è¯•æå–åˆ—è®¡ç®—ä»£ç 
        df_assignment_pattern = r'df\s*\[\s*["\']([^"\']+)["\']\s*\]\s*=\s*[^\n;]+(?:\n|;)' 
        df_assignments = re.findall(df_assignment_pattern, response)
            
        if df_assignments:
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰åˆ—è®¡ç®—çš„å‡½æ•°
            function_code = 'def dynamic_calculation(df):\n'
            for line in response.split('\n'):
                if 'df[' in line and '=' in line:
                    function_code += f'    {line.strip()}\n'
            function_code += '    return df\n'
                
            return [{
                'name': 'dynamic_calculation',
                'description': 'ä»AIå“åº”ä¸­æå–çš„åŠ¨æ€è®¡ç®—å‡½æ•°',
                'implementation': function_code,
                'required_columns': []
            }]
            
        return []
        
    def _fix_json_format(self, json_str: str) -> str:
        """
        å°è¯•ä¿®å¤JSONæ ¼å¼é—®é¢˜
            
        Args:
            json_str: åŸå§‹JSONå­—ç¬¦ä¸²
            
        Returns:
            ä¿®å¤åçš„JSONå­—ç¬¦ä¸²
        """
        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
        # 1. ä¿®å¤æœªè½¬ä¹‰çš„å¼•å·
        import re
        # æ›¿æ¢å¯èƒ½åœ¨å­—ç¬¦ä¸²å€¼ä¸­æœªè½¬ä¹‰çš„å¼•å·
        json_str = re.sub(r'([^\\])"([^\\])', r'\1\\"\2', json_str)
            
        # 2. ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜
        json_str = re.sub(r'\}\s*\{', r'}, {', json_str)
            
        # 3. ç¡®ä¿å­—ç¬¦ä¸²å€¼è¢«æ­£ç¡®åŒ…å›´
        lines = json_str.split('\n')
        fixed_lines = []
        for line in lines:
            # ç®€å•ä¿®å¤ä¸€äº›å¸¸è§çš„æ ¼å¼é—®é¢˜
            fixed_lines.append(line.strip())
            
        return '\n'.join(fixed_lines)
    
    def _sort_functions_by_dependencies(self, functions: List[Dict[str, Any]], original_columns: List[str]) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ä¾èµ–å…³ç³»å¯¹å‡½æ•°è¿›è¡Œæ‹“æ‰‘æ’åº
            
        Args:
            functions: ç”Ÿæˆçš„å‡½æ•°åˆ—è¡¨
            original_columns: åŸå§‹æ•°æ®åˆ—å
            
        Returns:
            æ’åºåçš„å‡½æ•°åˆ—è¡¨
        """
        from collections import deque
            
        # æ„å»ºä¾èµ–å›¾
        graph = {}
        in_degree = {}
        func_name_to_func = {}
            
        # åˆå§‹åŒ–æ‰€æœ‰å‡½æ•°çš„å…¥åº¦ä¸º0
        for func in functions:
            func_name = func.get('name', f'func_{id(func)}')
            func_name_to_func[func_name] = func
            graph[func_name] = []
            in_degree[func_name] = 0
            
        # è®°å½•æ‰€æœ‰å·²çŸ¥åˆ—ï¼ˆåŸå§‹åˆ— + å‡½æ•°ç”Ÿæˆçš„åˆ—ï¼‰
        known_columns = set(original_columns)
        generated_columns = {}
        
        # ä¸ºæ¯ä¸ªå‡½æ•°è®°å½•ç”Ÿæˆçš„åˆ—å
        for func in functions:
            func_name = func.get('name', f'func_{id(func)}')
            # å°è¯•ä»å‡½æ•°å®ç°ä¸­æå–ç”Ÿæˆçš„åˆ—å
            func_implementation = func.get('implementation', '')
            import re
            # åŒ¹é… df['new_column'] = ... æˆ– df["new_column"] = ...
            # ä½¿ç”¨ä¸åŒçš„å¼•å·åˆ†éš”æ­£åˆ™è¡¨è¾¾å¼
            pattern = r"df\[\s*['\"]([^'\"]+)['\"]\s*\]\s*=\s*[^\n;]+"
            column_matches = re.findall(pattern, func_implementation)
            generated_col = None
            for match in column_matches:
                # å¤„ç†å•å¼•å·å’ŒåŒå¼•å·åŒ¹é…
                generated_col = match
                if generated_col:
                    generated_columns[generated_col] = func_name
                    known_columns.add(generated_col)  # å°†æ–°ç”Ÿæˆçš„åˆ—åŠ å…¥å·²çŸ¥åˆ—
                    break
                    
        # æ„å»ºä¾èµ–å…³ç³»
        for func in functions:
            func_name = func.get('name', f'func_{id(func)}')
            func_implementation = func.get('implementation', '')
                
            # ä»å‡½æ•°å®ç°ä¸­æå–æ‰€æœ‰å¼•ç”¨çš„åˆ—å
            # åŒ¹é… df['col_name'] æˆ– df["col_name"]
            ref_pattern = r"df\[\s*['\"]([^'\"]+)['\"]\s*\]"
            ref_matches = re.findall(ref_pattern, func_implementation)
            referenced_cols = []
            for match in ref_matches:
                col = match
                if col and col not in [generated_col]:  # æ’é™¤æ­£åœ¨ç”Ÿæˆçš„åˆ—
                    referenced_cols.append(col)
                
            # æ£€æŸ¥å‡½æ•°ä¾èµ–çš„åˆ—
            for req_col in referenced_cols:
                # å¦‚æœä¾èµ–åˆ—æ˜¯ç”±å…¶ä»–å‡½æ•°ç”Ÿæˆçš„ï¼Œæ·»åŠ ä¾èµ–å…³ç³»
                if req_col in generated_columns:
                    dependent_func = generated_columns[req_col]
                    if dependent_func != func_name:  # é¿å…è‡ªç¯
                        graph[dependent_func].append(func_name)
                        in_degree[func_name] += 1
                # å¦åˆ™ï¼Œå¦‚æœä¾èµ–åˆ—ä¸æ˜¯åŸå§‹åˆ—ï¼Œè®°å½•è­¦å‘Š
                elif req_col not in known_columns:
                    self.logger.warning(f"å‡½æ•° {func_name} ä¾èµ–æœªçŸ¥åˆ—: {req_col}")
            
        # æ‹“æ‰‘æ’åº
        result = []
        queue = deque([func_name for func_name, degree in in_degree.items() if degree == 0])
            
        while queue:
            current = queue.popleft()
            result.append(func_name_to_func[current])
                
            for neighbor in graph[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
            
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¯
        if len(result) != len(functions):
            self.logger.warning(f"å‡½æ•°ä¾èµ–å›¾ä¸­å­˜åœ¨ç¯ï¼Œæ— æ³•å®Œå…¨æ’åºï¼Œå°†ä½¿ç”¨åŸå§‹é¡ºåº")
            return functions
            
        self.logger.info(f"å‡½æ•°æ‹“æ‰‘æ’åºå®Œæˆï¼Œæ‰§è¡Œé¡ºåº: {[func.get('name', 'unknown') for func in result]}")
        return result
    
    def _execute_direct_calculation(self, df: pd.DataFrame, func: Dict[str, Any]) -> pd.DataFrame:
        """
        ç›´æ¥æ‰§è¡Œæ ¸å¿ƒè®¡ç®—é€»è¾‘ï¼Œé¿å…å‡½æ•°å®šä¹‰çš„ç¼©è¿›é—®é¢˜
    
        Args:
            df: è¾“å…¥æ•°æ®DataFrame
            func: å‡½æ•°å­—å…¸ï¼ŒåŒ…å«å‡½æ•°å®ç°å’Œå…ƒæ•°æ®
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        import pandas as pd
        import numpy as np
        
        try:
            temp_df = df.copy()
            func_implementation = func.get('implementation', '')
            
            # å¦‚æœå®ç°ä¸­åŒ…å«å…·ä½“çš„è®¡ç®—é€»è¾‘ï¼Œç›´æ¥æ‰§è¡Œ
            if func_implementation:
                # é¢„å¤„ç†å‡½æ•°å®ç°ï¼Œç¡®ä¿å¯¼å…¥è¯­å¥å­˜åœ¨
                processed_func = func_implementation
                if 'import pandas' not in processed_func:
                    processed_func = 'import pandas as pd\nimport numpy as np\nimport datetime\n' + processed_func
                
                # ä¿®å¤å¯èƒ½çš„è¯­æ³•é—®é¢˜
                processed_func = processed_func.replace('pd.np.', 'np.')
                processed_func = processed_func.replace('pd.np', 'np')
                
                # è‡ªåŠ¨æ³¨å…¥æ—¥æœŸè½¬æ¢æç¤ºï¼ˆç”¨äºè§£å†³.dtè®¿é—®å™¨é—®é¢˜ï¼‰
                if '.dt.' in processed_func and 'pd.to_datetime' not in processed_func:
                    # ä¸ºä½¿ç”¨.dtè®¿é—®å™¨çš„åˆ—æ·»åŠ æ—¥æœŸè½¬æ¢
                    import re
                    # æŸ¥æ‰¾æ‰€æœ‰ä½¿ç”¨.dtè®¿é—®å™¨çš„åˆ—
                    dt_matches = re.findall(r"df\[['\"](\w+)['\"]\]\.dt\.", processed_func)
                    for col in set(dt_matches):
                        # åœ¨å‡½æ•°å¼€å§‹å¤„æ·»åŠ æ—¥æœŸè½¬æ¢ä»£ç 
                        conversion_code = f"    df['{col}'] = pd.to_datetime(df['{col}'])\n"
                        # æ‰¾åˆ°å‡½æ•°å®šä¹‰è¡Œå¹¶æ’å…¥æ—¥æœŸè½¬æ¢
                        lines = processed_func.split('\n')
                        for i, line in enumerate(lines):
                            if line.strip().startswith('def '):
                                lines.insert(i + 1, conversion_code)
                                break
                        processed_func = '\n'.join(lines)
                
                # åˆ›å»ºæœ¬åœ°å‘½åç©ºé—´
                import numpy as np
                local_namespace = {
                    'pd': pd,
                    'np': np,
                    'df': temp_df
                }
                
                # æ‰§è¡Œå‡½æ•°å®šä¹‰
                exec(processed_func, {'pd': pd, 'np': np}, local_namespace)
                
                # è·å–å‡½æ•°å¯¹è±¡å¹¶æ‰§è¡Œ
                func_name = func.get('name', '')
                if func_name in local_namespace:
                    func_obj = local_namespace[func_name]
                    result_df = func_obj(df.copy())
                    
                    # ä¿æŒåŸå§‹æ—¥æœŸåˆ—çš„æ ¼å¼ä¸€è‡´ï¼ˆå°†è½¬æ¢åçš„æ—¥æœŸåˆ—æ¢å¤ä¸ºåŸå§‹æ ¼å¼ï¼‰
                    for col in df.columns:
                        if df[col].dtype == 'object':  # åŸå§‹ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                            # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ï¼ˆYYYY-MM-DDï¼‰
                            sample_vals = df[col].dropna().head(5)
                            if len(sample_vals) > 0:
                                is_date_str = all(isinstance(val, str) and 
                                                len(val) == 10 and 
                                                val.count('-') == 2 and
                                                val.replace('-', '').isdigit() 
                                                for val in sample_vals if pd.notna(val))
                                if is_date_str and col in result_df.columns and pd.api.types.is_datetime64_any_dtype(result_df[col]):
                                    # å°†æ—¥æœŸæ—¶é—´åˆ—è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
                                    result_df[col] = result_df[col].dt.strftime('%Y-%m-%d').replace('NaT', None)
                    
                    new_columns = list(set(result_df.columns) - set(df.columns))
                    if new_columns:
                        self.logger.info(f"ç›´æ¥è®¡ç®—æˆåŠŸç”Ÿæˆæ–°åˆ—: {new_columns}")
                        self.logger.info(f"ç›´æ¥è®¡ç®—æ‰§è¡Œåæ•°æ®å½¢çŠ¶: {result_df.shape}")
                    else:
                        self.logger.info(f"ç›´æ¥è®¡ç®—æœªç”Ÿæˆæ–°åˆ—")
                    
                    return result_df
            
            # å¦‚æœæ²¡æœ‰å®ç°æˆ–æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›åŸå§‹df
            return df
        except KeyError as e:
            self.logger.error(f"ç›´æ¥è®¡ç®—æ‰§è¡Œå¤±è´¥ï¼Œç¼ºå°‘ä¾èµ–åˆ—: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            # å¯¹äºKeyErrorï¼Œè®°å½•ç¼ºå¤±çš„ä¾èµ–é¡¹ï¼Œä½†ç»§ç»­å¤„ç†
            missing_column = str(e).strip("'\"")
            self.logger.info(f"æ£€æµ‹åˆ°ç¼ºå¤±åˆ—: {missing_column}ï¼Œå°†å°è¯•åœ¨åç»­å‡½æ•°ä¸­ç”Ÿæˆ")
            return df
        except Exception as e:
            self.logger.error(f"ç›´æ¥è®¡ç®—æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return df
    
    def _execute_simplified_function(self, df: pd.DataFrame, func: Dict[str, Any]) -> pd.DataFrame:
        """
        æ‰§è¡Œç®€åŒ–åçš„å‡½æ•°ï¼ŒåŒ…å«æ ¸å¿ƒè®¡ç®—é€»è¾‘
    
        Args:
            df: è¾“å…¥æ•°æ®DataFrame
            func: å‡½æ•°å­—å…¸ï¼ŒåŒ…å«å‡½æ•°å®ç°å’Œå…ƒæ•°æ®
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        func_name = func.get('name', 'unknown')
        func_implementation = func.get('implementation', '')
        
        try:
            self.logger.info(f"å°è¯•ä¿®å¤å¹¶æ‰§è¡ŒAIç”Ÿæˆçš„å‡½æ•°")
            
            # é¢„å¤„ç†å‡½æ•°å®ç°ï¼Œç¡®ä¿å¯¼å…¥è¯­å¥å­˜åœ¨
            processed_func = func_implementation
            if 'import pandas' not in processed_func:
                processed_func = 'import pandas as pd\nimport numpy as np\nimport datetime\n' + processed_func
            
            # ä¿®å¤å¯èƒ½çš„è¯­æ³•é—®é¢˜
            processed_func = processed_func.replace('pd.np.', 'np.')
            processed_func = processed_func.replace('pd.np', 'np')
            
            # è‡ªåŠ¨æ³¨å…¥æ—¥æœŸè½¬æ¢æç¤ºï¼ˆç”¨äºè§£å†³.dtè®¿é—®å™¨é—®é¢˜ï¼‰
            if '.dt.' in processed_func and 'pd.to_datetime' not in processed_func:
                # ä¸ºä½¿ç”¨.dtè®¿é—®å™¨çš„åˆ—æ·»åŠ æ—¥æœŸè½¬æ¢
                import re
                # æŸ¥æ‰¾æ‰€æœ‰ä½¿ç”¨.dtè®¿é—®å™¨çš„åˆ—
                dt_matches = re.findall(r"df\[['\"](\w+)['\"]\]\.dt\.", processed_func)
                for col in set(dt_matches):
                    # åœ¨å‡½æ•°å¼€å§‹å¤„æ·»åŠ æ—¥æœŸè½¬æ¢ä»£ç 
                    conversion_code = f"    df['{col}'] = pd.to_datetime(df['{col}'])\n"
                    # æ‰¾åˆ°å‡½æ•°å®šä¹‰è¡Œå¹¶æ’å…¥æ—¥æœŸè½¬æ¢
                    lines = processed_func.split('\n')
                    for i, line in enumerate(lines):
                        if line.strip().startswith('def '):
                            lines.insert(i + 1, conversion_code)
                            break
                    processed_func = '\n'.join(lines)
            
            self.logger.info(f"å¤„ç†åçš„å‡½æ•°: {processed_func[:200]}...")
            
            # æ‰§è¡Œå‡½æ•°
            import numpy as np
            local_namespace = {'pd': pd, 'np': np, 'df': df.copy()}
            exec(processed_func, {'pd': pd, 'np': np}, local_namespace)
            
            if func_name in local_namespace:
                func_obj = local_namespace[func_name]
                result_df = func_obj(df.copy())  # ä½¿ç”¨dfçš„å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®)
                
                # ä¿æŒåŸå§‹æ—¥æœŸåˆ—çš„æ ¼å¼ä¸€è‡´ï¼ˆå°†è½¬æ¢åçš„æ—¥æœŸåˆ—æ¢å¤ä¸ºåŸå§‹æ ¼å¼ï¼‰
                for col in df.columns:
                    if df[col].dtype == 'object':  # åŸå§‹ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                        # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ï¼ˆYYYY-MM-DDï¼‰
                        sample_vals = df[col].dropna().head(5)
                        if len(sample_vals) > 0:
                            is_date_str = all(isinstance(val, str) and 
                                            len(val) == 10 and 
                                            val.count('-') == 2 and
                                            val.replace('-', '').isdigit() 
                                            for val in sample_vals if pd.notna(val))
                            if is_date_str and col in result_df.columns and pd.api.types.is_datetime64_any_dtype(result_df[col]):
                                # å°†æ—¥æœŸæ—¶é—´åˆ—è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
                                result_df[col] = result_df[col].dt.strftime('%Y-%m-%d').replace('NaT', None)
                
                if isinstance(result_df, pd.DataFrame):
                    new_columns = list(set(result_df.columns) - set(df.columns))
                    if new_columns:
                        self.logger.info(f"ç®€åŒ–å‡½æ•°æˆåŠŸç”Ÿæˆæ–°åˆ—: {new_columns}")
                    else:
                        self.logger.info(f"ç®€åŒ–å‡½æ•°æœªç”Ÿæˆæ–°åˆ—")
                    return result_df
                else:
                    self.logger.warning(f"å‡½æ•° {func_name} æœªè¿”å›DataFrameï¼Œè¿”å›ç±»å‹: {type(result_df)}")
                    return df
            else:
                self.logger.warning(f"å‡½æ•° {func_name} æœªåœ¨å‘½åç©ºé—´ä¸­æ‰¾åˆ°")
                return df
        except KeyError as e:
            self.logger.error(f"å‡½æ•°æ‰§è¡Œå¤±è´¥ï¼Œç¼ºå°‘ä¾èµ–åˆ—: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            # å¯¹äºKeyErrorï¼Œè®°å½•ç¼ºå¤±çš„ä¾èµ–é¡¹ï¼Œä»¥ä¾¿åç»­å¤„ç†
            missing_column = str(e).strip("'\"")
            self.logger.info(f"æ£€æµ‹åˆ°ç¼ºå¤±åˆ—: {missing_column}ï¼Œå°†å°è¯•åœ¨åç»­å‡½æ•°ä¸­ç”Ÿæˆ")
            return df
        except Exception as e:
            self.logger.error(f"ç®€åŒ–å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        
        return df
    
    def _report_error(self, requirement: str, func: Dict[str, Any], error: Exception, traceback_str: str, attempt: int = 1, df: pd.DataFrame = None):
        """
        ç»Ÿä¸€æŠ¥å‘Šé”™è¯¯ç»™AIæœåŠ¡

        Args:
            requirement: ç”¨æˆ·éœ€æ±‚
            func: å‡½æ•°å­—å…¸
            error: é”™è¯¯å¼‚å¸¸
            traceback_str: å †æ ˆè·Ÿè¸ªå­—ç¬¦ä¸²
            attempt: å°è¯•æ¬¡æ•°
            df: å½“å‰æ•°æ®å¸§ï¼Œç”¨äºè·å–åˆ—åä¿¡æ¯
        """
        # é”™è¯¯åˆ†ç±»
        error_type = type(error).__name__
        error_msg = str(error)
        
        # é’ˆå¯¹KeyErrorè¿›è¡Œç‰¹æ®Šå¤„ç†
        if error_type == "KeyError":
            missing_column = error_msg.strip("'\"")
            available_columns = list(df.columns) if df is not None else []
            error_msg = f"åˆ—åé”™è¯¯ï¼ç¼ºå°‘åˆ— '{missing_column}'ã€‚å½“å‰å¯ç”¨åˆ—ï¼š{available_columns}"
        
        # é’ˆå¯¹ValueErrorè¿›è¡Œç‰¹æ®Šå¤„ç†
        elif error_type == "ValueError":
            error_msg = f"å€¼é”™è¯¯ï¼è¯·æ£€æŸ¥æ•°æ®ç±»å‹å’Œè®¡ç®—é€»è¾‘ã€‚åŸå§‹é”™è¯¯ï¼š{error_msg}"
        
        # é’ˆå¯¹TypeErrorè¿›è¡Œç‰¹æ®Šå¤„ç†
        elif error_type == "TypeError":
            error_msg = f"ç±»å‹é”™è¯¯ï¼è¯·æ£€æŸ¥å‡½æ•°è°ƒç”¨å’Œå‚æ•°ç±»å‹ã€‚åŸå§‹é”™è¯¯ï¼š{error_msg}"
        
        ai_service = self._get_ai_service()
        ai_service.add_error({
            "prompt": requirement,
            "error": error_msg,
            "error_type": error_type,
            "implementation": func.get('implementation', ''),
            "function_name": func.get('name', 'unknown'),
            "traceback": traceback_str,
            "attempt": attempt,
            "available_columns": list(df.columns) if df is not None else []
        })
        
        # æ ¸å¿ƒï¼šå°†å¤±è´¥ç°åœºå­˜å…¥ Redis ä¾›ä¸‹æ¬¡ Prompt å‚è€ƒ
        if ai_service.qwen_learning:
            ai_service.qwen_learning.learn_from_error(
                prompt=requirement,
                enhanced_prompt=requirement,
                data_context={"columns": list(df.columns) if df is not None else []},
                error=error_msg,
                error_type=error_type,
                traceback=traceback_str,
                attempt=attempt
            )
    
    def process_data(self, df: pd.DataFrame, functions: List[Dict[str, Any]], requirement: str = "") -> pd.DataFrame:
        """
        åº”ç”¨ç”Ÿæˆçš„å‡½æ•°å¤„ç†æ•°æ®
    
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            functions: ç”Ÿæˆçš„å‡½æ•°åˆ—è¡¨
            requirement: ç”¨æˆ·éœ€æ±‚ï¼Œç”¨äºé”™è¯¯åé¦ˆ
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        try:
            processed_df = df.copy()
            original_columns = list(df.columns)
            
            # æ ¹æ®ä¾èµ–å…³ç³»å¯¹å‡½æ•°è¿›è¡Œæ‹“æ‰‘æ’åº
            sorted_functions = self._sort_functions_by_dependencies(functions, original_columns)
            
            self.logger.info(f"å¼€å§‹å¤„ç†æ•°æ®ï¼ŒåŸå§‹åˆ—: {original_columns}")
            self.logger.info(f"åŸå§‹æ•°æ®å½¢çŠ¶: {processed_df.shape}")
            
            for func in sorted_functions:
                func_name = func.get('name', 'unknown')
                func_implementation = func.get('implementation', '')
                func_description = func.get('description', '')
                required_columns = func.get('required_columns', [])
                
                self.logger.info(f"\n=== åº”ç”¨å‡½æ•°: {func_name} - {func_description} ===")
                self.logger.info(f"å‡½æ•°ä¾èµ–åˆ—: {required_columns}")
                
                # æ£€æŸ¥ä¾èµ–åˆ—æ˜¯å¦å­˜åœ¨
                missing_columns = []
                for req_col in required_columns:
                    if req_col not in processed_df.columns and req_col not in original_columns:
                        missing_columns.append(req_col)
                
                if missing_columns:
                    self.logger.warning(f"å‡½æ•° {func_name} ç¼ºå°‘ä¾èµ–åˆ—: {missing_columns}ï¼Œå°è¯•æ‰§è¡Œ...")
                else:
                    self.logger.info(f"å‡½æ•° {func_name} æ‰€éœ€ä¾èµ–åˆ—å‡å­˜åœ¨")
                
                # æ‰§è¡Œå‡½æ•°: å°è¯•å¤šç§ç­–ç•¥
                try:
                    # ç­–ç•¥1: æ‰§è¡Œç®€åŒ–åçš„å‡½æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨AIç”Ÿæˆçš„å‡½æ•°ï¼‰
                    self.logger.info(f"æ‰§è¡Œç­–ç•¥1: ç®€åŒ–å‡½æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨AIç”Ÿæˆçš„å‡½æ•°ï¼‰")
                    processed_df = self._execute_simplified_function(processed_df, func)
                    
                    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ–°åˆ—
                    new_columns_after_simplified = list(set(processed_df.columns) - set(df.columns))
                    if not new_columns_after_simplified:
                        # ç­–ç•¥2: ç›´æ¥æ‰§è¡Œæ ¸å¿ƒè®¡ç®—é€»è¾‘ï¼ˆä»…åœ¨ç®€åŒ–å‡½æ•°å¤±è´¥æ—¶å°è¯•ï¼‰
                        self.logger.info(f"ç®€åŒ–å‡½æ•°æœªç”Ÿæˆæ–°åˆ—ï¼Œå°è¯•ç­–ç•¥2: ç›´æ¥è®¡ç®—æ ¸å¿ƒé€»è¾‘")
                        processed_df = self._execute_direct_calculation(processed_df, func)
                except KeyError as e:
                    # å¤„ç†ä¾èµ–åˆ—ç¼ºå¤±çš„é”™è¯¯
                    missing_column = str(e)
                    self.logger.warning(f"å‡½æ•° {func_name} æ‰§è¡Œå¤±è´¥ï¼Œç¼ºå°‘ä¾èµ–åˆ—: {missing_column}")
                    
                    # è®°å½•ç¼ºå°‘çš„ä¾èµ–é¡¹ï¼Œä¾›åç»­è¿­ä»£ä½¿ç”¨
                    error_msg = f"ç¼ºå°‘ä¾èµ–åˆ—: {missing_column}"
                    import traceback
                    tb_str = traceback.format_exc()
                    self.logger.error(tb_str)
                    
                    # æŠ¥å‘Šé”™è¯¯ç»™AIæœåŠ¡
                    self._report_error(requirement, func, e, tb_str, df=processed_df)
                    
                    # è·³è¿‡å½“å‰å‡½æ•°ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªï¼ˆä¾èµ–é¡¹å¯èƒ½åœ¨å…¶ä»–å‡½æ•°ä¸­ç”Ÿæˆï¼‰
                    continue
                except Exception as e:
                    self.logger.error(f"å‡½æ•° {func_name} æ‰§è¡Œå¤±è´¥: {e}")
                    import traceback
                    tb_str = traceback.format_exc()
                    self.logger.error(tb_str)
                    
                    # æŠ¥å‘Šé”™è¯¯ç»™AIæœåŠ¡
                    self._report_error(requirement, func, e, tb_str, df=processed_df)
            
            final_columns = list(processed_df.columns)
            new_columns = list(set(final_columns) - set(original_columns))
            self.logger.info(f"\n=== æ•°æ®å¤„ç†å®Œæˆ ===")
            self.logger.info(f"åŸå§‹åˆ—æ•°: {len(original_columns)}, å¤„ç†ååˆ—æ•°: {len(final_columns)}")
            self.logger.info(f"æ–°å¢åˆ—: {new_columns}")
            self.logger.info(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {processed_df.shape}")
            
            return processed_df
        except Exception as e:
            self.logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return df
    
    def _validate_result(self, df: pd.DataFrame, processed_df: pd.DataFrame, requirement: str) -> tuple[bool, str]:
        """
        éªŒè¯å¤„ç†ç»“æœæ˜¯å¦ç¬¦åˆé€»è¾‘
        
        Args:
            df: åŸå§‹DataFrame
            processed_df: å¤„ç†åçš„DataFrame
            requirement: ç”¨æˆ·éœ€æ±‚
            
        Returns:
            (success, result_msg) å…ƒç»„ï¼Œsuccessä¸ºå¸ƒå°”å€¼ï¼Œresult_msgä¸ºç»“æœæè¿°
        """
        try:
            # åŸºæœ¬éªŒè¯ï¼šå¤„ç†åçš„DataFrameä¸èƒ½ä¸ºç©º
            if processed_df.empty:
                return False, "å¤„ç†åçš„æ•°æ®ä¸ºç©º"
            
            # åŸºæœ¬éªŒè¯ï¼šè¡Œæ•°ä¸åº”å‡å°‘
            if len(processed_df) < len(df):
                return False, f"å¤„ç†åè¡Œæ•°å‡å°‘: {len(processed_df)} < {len(df)}"
            
            # æ ¹æ®éœ€æ±‚ç±»å‹è¿›è¡Œç‰¹å®šéªŒè¯
            if "æ€»å’Œ" in requirement or "å¹³å‡å€¼" in requirement or "æœ€å¤§å€¼" in requirement or "æœ€å°å€¼" in requirement:
                # å¯¹äºè®¡ç®—ç±»éœ€æ±‚ï¼Œåº”è¯¥ç”Ÿæˆæ–°åˆ—
                new_columns = list(set(processed_df.columns) - set(df.columns))
                if not new_columns:
                    return False, f"è®¡ç®—ç±»éœ€æ±‚æœªç”Ÿæˆæ–°åˆ—"
            
            # éªŒè¯æ•°å€¼åˆ—çš„è®¡ç®—ç»“æœæ˜¯å¦åˆç†
            numeric_cols = df.select_dtypes(include=['number']).columns
            for col in numeric_cols:
                if f"{col}_æ€»å’Œ" in processed_df.columns:
                    # æ€»å’Œåˆ—çš„æ‰€æœ‰å€¼åº”è¯¥ç›¸åŒ
                    sum_values = processed_df[f"{col}_æ€»å’Œ"].unique()
                    if len(sum_values) > 1:
                        return False, f"æ€»å’Œåˆ— {col}_æ€»å’Œ åŒ…å«å¤šä¸ªä¸åŒå€¼"
            
            return True, "éªŒè¯é€šè¿‡"
        except Exception as e:
            return False, f"éªŒè¯å¤±è´¥: {str(e)}"
    
    def process_multi_columns(self, file_path, requirement, max_iterations=3):
        """
        å¤šåˆ—å¤„ç†ä¸»å‡½æ•°ï¼ŒåŒ…å«è¿­ä»£éªŒè¯é€»è¾‘
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            requirement: ç”¨æˆ·éœ€æ±‚
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # åŠ è½½æ•°æ®
            df = pd.read_excel(file_path)
            current_iter = 0
            last_feedback = ""
            
            # è®°å½•åŸå§‹åˆ—åï¼Œç”¨äºé”™è¯¯åˆ†ç±»
            original_columns = list(df.columns)
            
            while current_iter < max_iterations:
                self.logger.info(f"\n=== è¿­ä»£ {current_iter+1}/{max_iterations} ===")
                current_iter += 1
                
                # 1. è·å–å¢å¼ºåçš„æç¤ºè¯ï¼ˆåŒ…å« Redis ä¸­çš„å†å²é¿å‘æŒ‡å—ï¼‰
                data_context = self._generate_data_context(df)
                
                # 2. AI ç”Ÿæˆä»£ç 
                functions = self.generate_multi_column_functions(requirement, data_context, last_feedback, current_iter, max_iterations)
                
                if not functions:
                    last_feedback = "AIæœªç”Ÿæˆæœ‰æ•ˆçš„å‡½æ•°"
                    self.logger.warning(last_feedback)
                    continue
                
                # 3. åº”ç”¨å‡½æ•°å¤„ç†æ•°æ®
                processed_df = self.process_data(df, functions, requirement)
                
                # 4. éªŒè¯é˜¶æ®µ
                success, result_msg = self._validate_result(df, processed_df, requirement)
                
                # 5. å­¦ä¹ ä¸åé¦ˆ
                ai_service = self._get_ai_service()
                if success:
                    # è¿­ä»£æˆåŠŸï¼šè®°å½•ç»éªŒå¹¶é€€å‡º
                    for func in functions:
                        if ai_service.qwen_learning:
                            ai_service.qwen_learning.learn_from_iteration(
                                requirement=requirement,
                                code=func.get('implementation', ''),
                                success=True
                            )
                    
                    # å°†æˆåŠŸæ¡ˆä¾‹å­˜å…¥Redis
                    success_case = {
                        "requirement": requirement,
                        "columns": original_columns,
                        "code": [func.get('implementation', '') for func in functions],
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    try:
                        self.dual_redis.db_conn.set(f"success_cases:{requirement[:50]}", json.dumps([func.get('implementation', '') for func in functions]))
                        self.logger.info("æˆåŠŸæ¡ˆä¾‹å·²å­˜å…¥Redis")
                    except Exception as e:
                        self.logger.warning(f"å­˜å…¥æˆåŠŸæ¡ˆä¾‹å¤±è´¥: {e}")
                    
                    # æ¸…ç©ºæœ€åå¤±è´¥ä»£ç è®°å½•
                    self.last_failed_code = []
                    
                    # ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
                    import tempfile
                    import os
                    temp_dir = tempfile.gettempdir()
                    temp_file = os.path.join(temp_dir, f"processed_{os.path.basename(file_path)}")
                    processed_df.to_excel(temp_file, index=False)
                    
                    return {
                        "success": True,
                        "file_path": temp_file,
                        "data": processed_df,
                        "message": f"æˆåŠŸï¼Œè¿­ä»£æ¬¡æ•°: {current_iter}",
                        "new_columns": list(set(processed_df.columns) - set(df.columns))
                    }
                else:
                    # è¿­ä»£å¤±è´¥ï¼šè®°å½•é”™è¯¯ï¼Œåé¦ˆç»™ä¸‹ä¸€è½® AI
                    last_feedback = result_msg
                    
                    # é”™è¯¯åˆ†ç±»ï¼šé’ˆå¯¹ä¸åŒç±»å‹çš„é”™è¯¯æä¾›æ›´å…·ä½“çš„åé¦ˆ
                    if "KeyError" in last_feedback:
                        # åˆ—åé”™è¯¯ï¼šæä¾›æ˜ç¡®çš„åˆ—åæ¸…å•
                        last_feedback = f"åˆ—åé”™è¯¯ï¼å½“å‰å¯ç”¨åˆ—æœ‰ï¼š{original_columns}ï¼Œè¯·é‡æ–°åŒ¹é…ã€‚\nåŸå§‹é”™è¯¯ï¼š{result_msg}"
                    elif "ValueError" in last_feedback:
                        # å€¼é”™è¯¯ï¼šæç¤ºæ•°æ®ç±»å‹é—®é¢˜
                        last_feedback = f"å€¼é”™è¯¯ï¼è¯·æ£€æŸ¥æ•°æ®ç±»å‹å’Œè®¡ç®—é€»è¾‘æ˜¯å¦åŒ¹é…ã€‚\nåŸå§‹é”™è¯¯ï¼š{result_msg}"
                    elif "TypeError" in last_feedback:
                        # ç±»å‹é”™è¯¯ï¼šæç¤ºå‡½æ•°è°ƒç”¨å’Œå‚æ•°é—®é¢˜
                        last_feedback = f"ç±»å‹é”™è¯¯ï¼è¯·æ£€æŸ¥å‡½æ•°è°ƒç”¨å’Œå‚æ•°ç±»å‹æ˜¯å¦æ­£ç¡®ã€‚\nåŸå§‹é”™è¯¯ï¼š{result_msg}"
                    elif "MultiIndex" in last_feedback and "index=False" in last_feedback:
                        # MultiIndex å¯¼å‡ºé—®é¢˜
                        last_feedback = f"MultiIndex å¯¼å‡ºé”™è¯¯ï¼å½“å­˜åœ¨ MultiIndex æ—¶ï¼Œå¿…é¡»è®¾ç½® index=Trueï¼Œæˆ–å¹³é“ºè¡¨å¤´ã€‚\nåŸå§‹é”™è¯¯ï¼š{result_msg}"
                    elif "Timedelta" in last_feedback:
                        # æ—¶é—´å·®è¿ç®—é—®é¢˜
                        last_feedback = f"æ—¶é—´å·®è¿ç®—é”™è¯¯ï¼å¯¹æ—¶é—´å·®è¿›è¡Œè¿ç®—å‰ï¼Œå¿…é¡»ä½¿ç”¨ .dt.total_seconds() è½¬æ¢ä¸ºæ•°å€¼ã€‚\nåŸå§‹é”™è¯¯ï¼š{result_msg}"
                    elif "UFuncNoLoopError" in last_feedback or "dtype('<U4')" in last_feedback:
                        # ç±»å‹æ··åˆé—®é¢˜
                        last_feedback = f"ç±»å‹æ··åˆé”™è¯¯ï¼æ£€æµ‹åˆ°å­—ç¬¦ä¸²ä¸æ•°å­—æ··åˆï¼Œè¯·å…ˆæ‰§è¡Œ df.fillna(0) å¹¶å¼ºåˆ¶è½¬æ¢ç±»å‹ã€‚\nåŸå§‹é”™è¯¯ï¼š{result_msg}"
                    
                    # è®°å½•è¿­ä»£é”™è¯¯
                    for func in functions:
                        if ai_service.qwen_learning:
                            ai_service.qwen_learning.learn_from_iteration(
                                requirement=requirement,
                                code=func.get('implementation', ''),
                                error_msg=last_feedback,
                                success=False
                            )
                    
                    # æ ¸å¿ƒï¼šå°†å¤±è´¥ç°åœºå­˜å…¥ Redis ä¾›ä¸‹æ¬¡ Prompt å‚è€ƒ
                    if ai_service.qwen_learning:
                        ai_service.qwen_learning.learn_from_failure(
                            requirement=requirement,
                            error_msg=last_feedback
                        )
                    
                    # è®°å½•æœ€åä¸€æ¬¡å¤±è´¥çš„ä»£ç 
                    self.last_failed_code = [func.get('implementation', '') for func in functions]
                    
                    self.logger.warning(f"è¿­ä»£å¤±è´¥: {last_feedback}")
                    
            return {
                "success": False,
                "message": f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå¤„ç†å¤±è´¥",
                "last_feedback": last_feedback
            }
        except Exception as e:
            self.logger.error(f"process_multi_columns å¤±è´¥: {e}")
            import traceback
            traceback_str = traceback.format_exc()
            self.logger.error(traceback_str)
            
            # é”™è¯¯åˆ†ç±»ï¼šé’ˆå¯¹ä¸åŒç±»å‹çš„é”™è¯¯æä¾›æ›´å…·ä½“çš„åé¦ˆ
            error_msg = str(e)
            if "KeyError" in error_msg:
                # åˆ—åé”™è¯¯ï¼šæä¾›æ˜ç¡®çš„åˆ—åæ¸…å•
                error_msg = f"åˆ—åé”™è¯¯ï¼å½“å‰å¯ç”¨åˆ—æœ‰ï¼š{list(df.columns) if 'df' in locals() else []}ï¼Œè¯·é‡æ–°åŒ¹é…ã€‚\nåŸå§‹é”™è¯¯ï¼š{error_msg}"
            elif "ValueError" in error_msg:
                # å€¼é”™è¯¯ï¼šæç¤ºæ•°æ®ç±»å‹é—®é¢˜
                error_msg = f"å€¼é”™è¯¯ï¼è¯·æ£€æŸ¥æ•°æ®ç±»å‹å’Œè®¡ç®—é€»è¾‘æ˜¯å¦åŒ¹é…ã€‚\nåŸå§‹é”™è¯¯ï¼š{error_msg}"
            elif "TypeError" in error_msg:
                # ç±»å‹é”™è¯¯ï¼šæç¤ºå‡½æ•°è°ƒç”¨å’Œå‚æ•°é—®é¢˜
                error_msg = f"ç±»å‹é”™è¯¯ï¼è¯·æ£€æŸ¥å‡½æ•°è°ƒç”¨å’Œå‚æ•°ç±»å‹æ˜¯å¦æ­£ç¡®ã€‚\nåŸå§‹é”™è¯¯ï¼š{error_msg}"
            
            return {
                "success": False,
                "message": f"å¤„ç†å¤±è´¥: {error_msg}",
                "traceback": traceback_str
            }
    
    def _analyze_dependencies_phase(self, data_context: Dict, requirement: str):
        """
        ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æä¾èµ–å…³ç³» - è®©AIåªè¾“å‡ºä¾èµ–å…³ç³»è¡¨
        """
        # é¦–å…ˆå°è¯•ä½¿ç”¨è‹±æ–‡æç¤ºï¼Œé¿å…ä¸­æ–‡å­—ç¬¦å¯¼è‡´çš„è§£æé—®é¢˜
        analysis_prompt = self._create_dependency_analysis_prompt(data_context, requirement)
        
        print("ğŸ”„ æ‰§è¡Œç¬¬ä¸€é˜¶æ®µï¼šä¾èµ–å…³ç³»åˆ†æ...")
        
        # è·å–AIæœåŠ¡å®ä¾‹
        ai_service = self._get_ai_service()
        
        # ä½¿ç”¨process_single_cellæ–¹æ³•ç›´æ¥è·å–AIæ–‡æœ¬å“åº”
        success, response_text, error_message = ai_service.api_manager.process_single_cell(
            cell_content=analysis_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œä¸“é—¨åˆ†ææ•°æ®å¤„ç†éœ€æ±‚ä¸­çš„ä¾èµ–å…³ç³»ã€‚è¯·ä½¿ç”¨è‹±æ–‡åˆ—åï¼Œä¸è¦ä½¿ç”¨ä¸­æ–‡åˆ—åã€‚",
            user_prompt="è¯·åˆ†æä»¥ä¸‹æ•°æ®å¤„ç†éœ€æ±‚çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚",
            max_tokens=1000
        )
        
        if success and response_text:
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            dependency_analysis = self._extract_dependency_analysis_from_response(response_text)
            if dependency_analysis:
                print("âœ… ä¾èµ–å…³ç³»åˆ†æå®Œæˆ:")
                for item in dependency_analysis.get('dependency_analysis', []):
                    print(f"   - {item['column_name']}: ä¾èµ–äº {item['depends_on']}")
                return dependency_analysis
        
        print(f"âŒ AIåˆ†æå¤±è´¥æˆ–è¿”å›æ— æ•ˆæ ¼å¼: {error_message if not success else 'è§£æå¤±è´¥'}")
        return None
    
    def _create_dependency_analysis_prompt(self, data_context: Dict, requirement: str) -> str:
        """
        åˆ›å»ºä¾èµ–å…³ç³»åˆ†ææç¤ºè¯
        """
        # æ„å»ºåˆ†æé˜¶æ®µçš„æç¤º
        analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ•°æ®å¤„ç†éœ€æ±‚çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›æ¯åˆ—çš„ä¾èµ–é¡¹ã€‚

æ•°æ®åˆ—: {data_context['columns']}
æ•°æ®ç±»å‹: {data_context['data_types']}

å¤„ç†éœ€æ±‚: {requirement}

é‡è¦ï¼šå¿…é¡»ä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„ä¸­æ–‡åˆ—åï¼Œä¸è¦åˆ›å»ºæ–°çš„è‹±æ–‡åˆ—åã€‚ä½¿ç”¨æ•°æ®ä¸­å­˜åœ¨çš„åˆ—åï¼š{data_context['columns']}ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›JSON:
{{
    "dependency_analysis": [
        {{
            "column_name": "éœ€è¦ç”Ÿæˆçš„åˆ—åï¼ˆä½¿ç”¨ä¸­æ–‡æˆ–éœ€æ±‚ä¸­æŒ‡å®šçš„è‹±æ–‡åï¼‰",
            "description": "åˆ—çš„è®¡ç®—æè¿°",
            "depends_on": ["ä¾èµ–çš„åˆ—ååˆ—è¡¨ï¼ˆå¿…é¡»æ˜¯åŸå§‹æ•°æ®ä¸­å­˜åœ¨çš„åˆ—åï¼‰"],
            "calculation_type": "è®¡ç®—ç±»å‹ï¼Œå¦‚æ¡ä»¶è®¡ç®—ã€æ•°å­¦è¿ç®—ç­‰"
        }}
    ]
}}

é‡è¦ï¼šåªè¿”å›JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""
        return analysis_prompt
    
    def _extract_dependency_analysis_from_response(self, response_text: str):
        """
        ä»AIå“åº”ä¸­æå–ä¾èµ–å…³ç³»åˆ†æ
        """
        import re
        import json
        
        # å°è¯•ç›´æ¥è§£æå®Œæ•´JSON
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            pass
        
        # æŸ¥æ‰¾JSONéƒ¨åˆ†
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                json_str = self._fix_json_format(json_str)
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    pass
        
        # å¦‚æœç›´æ¥JSONè§£æå¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾æ•°ç»„éƒ¨åˆ†
        array_match = re.search(r'\[.*\]', response_text, re.DOTALL)
        if array_match:
            array_str = array_match.group()
            try:
                # å°†æ•°ç»„åŒ…è£…æˆå®Œæ•´çš„JSONå¯¹è±¡
                wrapped_json = f'{{"dependency_analysis": {array_str}}}'
                return json.loads(wrapped_json)
            except json.JSONDecodeError:
                pass
        
        print(f"âŒ æ— æ³•ä»AIå“åº”ä¸­æå–JSON: {response_text[:200]}...")
        return None
        # æ„å»ºåˆ†æé˜¶æ®µçš„æç¤º
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ•°æ®å¤„ç†éœ€æ±‚çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›æ¯åˆ—çš„ä¾èµ–é¡¹ã€‚

æ•°æ®åˆ—: {data_context['columns']}
æ•°æ®ç±»å‹: {data_context['data_types']}

å¤„ç†éœ€æ±‚: {requirement}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›JSON:
{{
    "dependency_analysis": [
        {{
            "column_name": "åˆ—å",
            "description": "åˆ—çš„è®¡ç®—æè¿°",
            "depends_on": ["ä¾èµ–çš„åˆ—ååˆ—è¡¨", "å¯ä»¥åŒ…å«åŸå§‹æ•°æ®åˆ—æˆ–éœ€è¦æ–°ç”Ÿæˆçš„åˆ—"],
            "calculation_type": "è®¡ç®—ç±»å‹ï¼Œå¦‚æ¡ä»¶è®¡ç®—ã€æ•°å­¦è¿ç®—ç­‰"
        }}
    ]
}}

é‡è¦ï¼šåªè¿”å›JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        print("ğŸ”„ æ‰§è¡Œç¬¬ä¸€é˜¶æ®µï¼šä¾èµ–å…³ç³»åˆ†æ...")
        
        # è·å–AIæœåŠ¡å®ä¾‹
        ai_service = self._get_ai_service()
        
        # ä½¿ç”¨process_single_cellæ–¹æ³•ç›´æ¥è·å–AIæ–‡æœ¬å“åº”
        success, response_text, error_message = ai_service.api_manager.process_single_cell(
            cell_content=analysis_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œä¸“é—¨åˆ†ææ•°æ®å¤„ç†éœ€æ±‚ä¸­çš„ä¾èµ–å…³ç³»ã€‚",
            user_prompt="è¯·åˆ†æä»¥ä¸‹æ•°æ®å¤„ç†éœ€æ±‚çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚",
            max_tokens=1000
        )
        
        if success and response_text:
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            try:
                # æŸ¥æ‰¾JSONéƒ¨åˆ†
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    dependency_analysis = json.loads(json_str)
                    print("âœ… ä¾èµ–å…³ç³»åˆ†æå®Œæˆ:")
                    for item in dependency_analysis.get('dependency_analysis', []):
                        print(f"   - {item['column_name']}: ä¾èµ–äº {item['depends_on']}")
                    return dependency_analysis
                else:
                    print(f"âŒ æ— æ³•ä»AIå“åº”ä¸­æå–JSON: {response_text}")
                    return None
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æé”™è¯¯: {e}")
                print(f"AIå“åº”: {response_text}")
                return None
        else:
            print(f"âŒ AIåˆ†æå¤±è´¥: {error_message}")
            return None

    def _sort_dependencies_with_original_columns(self, dependency_analysis: Dict, original_columns: List[str]):
        """
        æ”¹è¿›çš„æ‹“æ‰‘æ’åº - å»ºç«‹"åŸå§‹åˆ—ç™½åå•"
        """
        print("ğŸ”„ æ‰§è¡Œç¬¬äºŒé˜¶æ®µï¼šä¾èµ–å…³ç³»æ’åºï¼ˆè€ƒè™‘åŸå§‹åˆ—ï¼‰...")
        
        # åŒºåˆ†åŸå§‹åˆ—å’Œéœ€è¦ç”Ÿæˆçš„æ–°åˆ—
        original_set = set(original_columns)
        
        # æ„å»ºä¾èµ–å›¾ï¼Œä½†åªå¯¹æ–°åˆ—è¿›è¡Œæ’åº
        nodes = {}
        dependencies = {}
        
        for item in dependency_analysis.get('dependency_analysis', []):
            col_name = item['column_name']
            depends_on = item.get('depends_on', [])
            
            # åªå¯¹éœ€è¦ç”Ÿæˆçš„åˆ—ï¼ˆéåŸå§‹åˆ—ï¼‰å»ºç«‹ä¾èµ–å›¾
            if col_name not in original_set:
                nodes[col_name] = item
                dependencies[col_name] = depends_on
        
        # æ‹“æ‰‘æ’åº - åªå¯¹éœ€è¦ç”Ÿæˆçš„åˆ—è¿›è¡Œæ’åº
        sorted_new_columns = []
        visited = set()
        temp_visited = set()
        
        def visit(node):
            if node in temp_visited:
                raise ValueError(f"å¾ªç¯ä¾èµ–: {node}")
            if node in visited:
                return
                
            temp_visited.add(node)
            
            # è®¿é—®æ‰€æœ‰ä¾èµ–é¡¹
            for dep in dependencies.get(node, []):
                # åªè®¿é—®éœ€è¦ç”Ÿæˆçš„åˆ—ï¼ˆéåŸå§‹åˆ—ï¼‰
                if dep not in original_set and dep not in visited:
                    visit(dep)
            
            temp_visited.remove(node)
            visited.add(node)
            sorted_new_columns.append(node)
        
        # å¯¹æ‰€æœ‰éœ€è¦ç”Ÿæˆçš„èŠ‚ç‚¹è¿›è¡Œè®¿é—®
        for node in dependencies.keys():
            if node not in visited:
                try:
                    visit(node)
                except ValueError as e:
                    print(f"âŒ æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {e}")
                    return None
        
        print("âœ… æ‹“æ‰‘æ’åºå®Œæˆ:")
        print("   ç°è‰²ï¼ˆåŸå§‹åˆ—ï¼‰:", list(original_set))
        for i, col in enumerate(sorted_new_columns):
            print(f"   è“è‰²ï¼ˆç”Ÿæˆåˆ—ï¼‰Level {i}: {col}")
        
        return {
            'original_columns': list(original_set),
            'new_columns': sorted_new_columns
        }

    def get_last_failed_code(self):
        """
        è·å–æœ€åä¸€æ¬¡å¤±è´¥çš„ä»£ç ï¼Œç”¨äº Gradio å‰ç«¯å±•ç¤ºå’Œäººå·¥ä¿®æ­£
        
        Returns:
            æœ€åä¸€æ¬¡å¤±è´¥çš„ä»£ç åˆ—è¡¨
        """
        return self.last_failed_code
    
    def _closed_loop_execute_and_validate(self, df: pd.DataFrame, requirement: str, sorted_result: Dict, dependency_analysis: Dict):
        """
        é—­ç¯è®­ç»ƒï¼šåŸå­åŒ–æ‰§è¡Œ + ä¸­é—´çŠ¶æ€ä¼ é€’ + ç¡¬æ€§çº¦æŸ
        """
        print("ğŸ”„ æ‰§è¡Œç¬¬ä¸‰é˜¶æ®µï¼šé—­ç¯è®­ç»ƒåŸå­åŒ–é¡ºåºæ‰§è¡Œä¸éªŒè¯...")
        
        # è·å–AIæœåŠ¡å®ä¾‹
        ai_service = self._get_ai_service()
        
        # åˆå§‹åŒ–æ‰§è¡ŒçŠ¶æ€
        executed_columns = set(df.columns)  # åˆå§‹å¯ç”¨åˆ— = åŸå§‹åˆ—
        executed_functions = []
        
        # è®°å½•æ¯ä¸€æ­¥çš„ä¸­é—´çŠ¶æ€
        intermediate_states = {}
        
        print(f"   ğŸŸ¨ ç°è‰²ï¼šåŸå§‹åˆ—å·²å°±ç»ª - {list(df.columns)}")
        
        # é€ä¸ªæ‰§è¡Œéœ€è¦ç”Ÿæˆçš„æ–°åˆ—ï¼ˆè“è‰²ï¼‰
        for level, col_name in enumerate(sorted_result['new_columns']):
            print(f"\n--- Level {level}: å¤„ç†è“è‰²åˆ— {col_name} ---")
            
            # è·å–åˆ—ä¿¡æ¯
            col_info = None
            for item in dependency_analysis.get('dependency_analysis', []):
                if item['column_name'] == col_name:
                    col_info = item
                    break
            
            if not col_info:
                print(f"   âŒ æœªæ‰¾åˆ°åˆ—ä¿¡æ¯: {col_name}")
                continue
            
            # æ£€æŸ¥ä¾èµ–æ˜¯å¦éƒ½å·²æ»¡è¶³ï¼ˆåªè€ƒè™‘éœ€è¦ç”Ÿæˆçš„åˆ—ï¼‰
            required_deps = set(col_info.get('depends_on', []))
            missing_deps = required_deps - executed_columns
            
            if missing_deps:
                print(f"   âŒ ä¾èµ–æœªæ»¡è¶³: {col_name} éœ€è¦ {missing_deps}ï¼Œä½†åªæœ‰ {executed_columns}")
                
                # è®°å½•ä¾èµ–é¡ºåºé”™è¯¯åˆ°æ•°æ®åº“ï¼ˆåŒ…å«æ‹“æ‰‘å›¾ä¿¡æ¯ï¼‰
                self._record_dependency_error_with_topology(ai_service, requirement, col_name, missing_deps, executed_columns, sorted_result)
                continue
            
            # ç”Ÿæˆé’ˆå¯¹å•ä¸ªåˆ—çš„å‡½æ•°
            import re
            safe_col_name = re.sub(r'[^\w]', '_', col_name.lower())
            
            # å¼ºåˆ¶ä¼ é€’ä¸­é—´å¿«ç…§ï¼šåŒ…å«å·²æ‰§è¡Œçš„åˆ—ä¿¡æ¯
            executed_state_info = ""
            if intermediate_states:
                executed_state_info = f"\nå·²æ‰§è¡Œçš„åˆ—çŠ¶æ€:\n"
                for executed_col, state in intermediate_states.items():
                    executed_state_info += f"- {executed_col}: {state}\n"
            
            function_prompt = f"""
ä¸ºä»¥ä¸‹æ•°æ®åˆ—ç”Ÿæˆå¤„ç†å‡½æ•°ï¼š

æ•°æ®ä¸Šä¸‹æ–‡:
- å½“å‰å¯ç”¨åˆ—: {list(executed_columns)}
- éœ€è¦ç”Ÿæˆçš„åˆ—: {col_name}
- åˆ—æè¿°: {col_info.get('description', '')}
- è®¡ç®—ç±»å‹: {col_info.get('calculation_type', '')}
- ä¾èµ–åˆ—: {col_info.get('depends_on', [])}
{executed_state_info}

åŸå§‹éœ€æ±‚: {requirement}

é‡è¦çº¦æŸæ¡ä»¶:
1. ä¸¥ç¦åœ¨å‡½æ•°å†…æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå› ä¸ºç³»ç»Ÿå·²åœ¨é¢„æ£€é˜¶æ®µç¡®è®¤ä¾èµ–åˆ—å­˜åœ¨
2. ç›´æ¥ä½¿ç”¨ä¾èµ–åˆ—ï¼Œä¸è¦æ·»åŠ  if 'åˆ—å' in df ç­‰æ£€æŸ¥é€»è¾‘
3. ä½¿ç”¨æ ‡å‡†çš„pandaså’Œnumpyè¯­æ³•ï¼Œä¸è¦ä½¿ç”¨pd.np
4. å¿…é¡»ä½¿ç”¨éœ€æ±‚ä¸­æŒ‡å®šçš„åˆ—åï¼Œä¿æŒä¸ä¾èµ–åˆ†æé˜¶æ®µçš„åˆ—åä¸€è‡´

è¯·ç”Ÿæˆä¸€ä¸ªPythonå‡½æ•°ï¼Œå®ç°ä»¥ä¸‹åŠŸèƒ½:
1. å‡½æ•°å: calculate_{safe_col_name}
2. è¾“å…¥: pandas DataFrame
3. è¾“å‡º: å¤„ç†åçš„DataFrameï¼ˆåŒ…å«æ–°åˆ—ï¼‰
4. ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹å’Œæ¡ä»¶åˆ¤æ–­

è¿”å›æ ¼å¼:
{{
    "name": "å‡½æ•°å",
    "description": "å‡½æ•°æè¿°",
    "implementation": "å‡½æ•°å®ç°ä»£ç ",
    "required_columns": ["éœ€è¦çš„åˆ—"],
    "new_columns": ["æ–°ç”Ÿæˆçš„åˆ—"]
}}"""
            
            print(f"   ç”Ÿæˆå‡½æ•°: {col_name}")
            
            # è·å–AIç”Ÿæˆçš„å‡½æ•°
            success, response_text, error_message = ai_service.api_manager.process_single_cell(
                cell_content=function_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„pandaså‡½æ•°ç”Ÿæˆå™¨ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®å¤„ç†å‡½æ•°ã€‚é‡è¦ï¼šä¸è¦æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨å·²ç¡®è®¤å­˜åœ¨çš„åˆ—ã€‚",
                user_prompt="è¯·ä¸ºæŒ‡å®šåˆ—ç”Ÿæˆå¤„ç†å‡½æ•°ï¼Œä¸è¦åŒ…å«åˆ—å­˜åœ¨æ€§æ£€æŸ¥ã€‚",
                max_tokens=800
            )
            
            if success and response_text:
                try:
                    # æå–JSON
                    json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                    if json_match:
                        func_data = json.loads(json_match.group())
                        
                        # éªŒè¯å‡½æ•°ç»“æ„
                        if all(key in func_data for key in ['name', 'implementation']):
                            # æ‰§è¡Œå‡½æ•°å¹¶éªŒè¯
                            success, result_df = self._execute_and_validate_with_state_tracking(df, func_data, col_name)
                            
                            if success and result_df is not None:
                                # æ›´æ–°DataFrameçŠ¶æ€
                                df = result_df
                                
                                # æ·»åŠ åˆ°å·²æ‰§è¡Œåˆ—
                                new_cols = func_data.get('new_columns', [col_name])
                                executed_columns.update(new_cols)
                                executed_functions.append(func_data)
                                
                                # è®°å½•ä¸­é—´çŠ¶æ€
                                for new_col in new_cols:
                                    if new_col in df.columns:
                                        intermediate_states[new_col] = str(df[new_col].head(2).tolist())
                                
                                print(f"     âœ… å‡½æ•°æ‰§è¡ŒæˆåŠŸ: {func_data['name']} -> ğŸŸ¢ ç»¿è‰²åˆ—")
                                print(f"     ğŸ“Š ä¸­é—´çŠ¶æ€: {col_name} = {intermediate_states.get(col_name, 'N/A')}")
                            else:
                                print(f"     âŒ å‡½æ•°æ‰§è¡ŒéªŒè¯å¤±è´¥: {func_data['name']}")
                                
                                # è®°å½•æ‰§è¡Œé”™è¯¯ï¼ˆåŒ…å«ä¸­é—´çŠ¶æ€ï¼‰
                                self._record_execution_error_with_state(ai_service, requirement, func_data, col_name, intermediate_states)
                        else:
                            print(f"     âŒ å‡½æ•°ç»“æ„ä¸å®Œæ•´: {func_data}")
                            
                            # è®°å½•ç»“æ„é”™è¯¯
                            self._record_structure_error(ai_service, requirement, func_data, col_name)
                    else:
                        print(f"     âŒ æ— æ³•æå–å‡½æ•°JSON: {response_text}")
                        
                        # è®°å½•è§£æé”™è¯¯
                        self._record_parsing_error(ai_service, requirement, response_text, col_name)
                except json.JSONDecodeError as e:
                    print(f"     âŒ å‡½æ•°JSONè§£æé”™è¯¯: {e}")
                    
                    # è®°å½•è§£æé”™è¯¯
                    self._record_parsing_error(ai_service, requirement, response_text, col_name)
            else:
                print(f"     âŒ å‡½æ•°ç”Ÿæˆå¤±è´¥: {error_message}")
                
                # è®°å½•ç”Ÿæˆé”™è¯¯
                self._record_generation_error(ai_service, requirement, col_name)
        
        print(f"\nâœ… é—­ç¯è®­ç»ƒåŸå­åŒ–æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸæ‰§è¡Œ {len(executed_functions)} ä¸ªå‡½æ•°")
        return executed_functions, df, intermediate_states

    def _execute_and_validate_with_state_tracking(self, df: pd.DataFrame, func_data: Dict, col_name: str):
        """
        æ‰§è¡Œå‡½æ•°å¹¶éªŒè¯ï¼ŒåŒæ—¶è®°å½•çŠ¶æ€
        """
        import signal
        import time
        
        def timeout_handler(signum, frame):
            raise TimeoutError("å‡½æ•°æ‰§è¡Œè¶…æ—¶")
        
        try:
            import pandas as pd
            import numpy as np
            func_impl = func_data.get('implementation', '')
            
            # å¼ºåˆ¶ç¯å¢ƒåˆå§‹åŒ–ï¼šé¢„ç½®æ­£ç¡®çš„å¯¼å…¥
            if 'import pandas' not in func_impl:
                func_impl = 'import pandas as pd\nimport numpy as np\nimport datetime\n' + func_impl
            
            # æ›¿æ¢å¯èƒ½çš„é”™è¯¯è¯­æ³•
            func_impl = func_impl.replace('pd.np.', 'np.')
            func_impl = func_impl.replace('pd.np', 'np')
            
            # è‡ªåŠ¨æ³¨å…¥æ—¥æœŸè½¬æ¢æç¤ºï¼ˆç”¨äºè§£å†³.dtè®¿é—®å™¨é—®é¢˜ï¼‰
            if '.dt.' in func_impl and 'pd.to_datetime' not in func_impl:
                # ä¸ºä½¿ç”¨.dtè®¿é—®å™¨çš„åˆ—æ·»åŠ æ—¥æœŸè½¬æ¢
                import re
                # æŸ¥æ‰¾æ‰€æœ‰ä½¿ç”¨.dtè®¿é—®å™¨çš„åˆ—
                dt_matches = re.findall(r"df\[['\"](\w+)['\"]\]\.dt\.", func_impl)
                for col in set(dt_matches):
                    # åœ¨å‡½æ•°å¼€å§‹å¤„æ·»åŠ æ—¥æœŸè½¬æ¢ä»£ç 
                    conversion_code = f"    df['{col}'] = pd.to_datetime(df['{col}'])\n"
                    # æ‰¾åˆ°å‡½æ•°å®šä¹‰è¡Œå¹¶æ’å…¥æ—¥æœŸè½¬æ¢
                    lines = func_impl.split('\n')
                    for i, line in enumerate(lines):
                        if line.strip().startswith('def '):
                            lines.insert(i + 1, conversion_code)
                            break
                    func_impl = '\n'.join(lines)
            
            # åˆ›å»ºæœ¬åœ°å‘½åç©ºé—´ï¼ˆé¢„ç½®ç¯å¢ƒè¡¥ä¸ï¼‰
            import numpy as np
            local_namespace = {
                'pd': pd,
                'np': np,
                'df': df.copy(),
                'datetime': pd.Timestamp,  # æ·»åŠ datetimeæ”¯æŒ
                'timedelta': pd.Timedelta  # æ·»åŠ timedeltaæ”¯æŒ
            }
            
            # æ‰§è¡Œå‡½æ•°å®šä¹‰
            exec(func_impl, {'pd': pd, 'np': np}, local_namespace)
            
            # è·å–å‡½æ•°å¯¹è±¡
            func_name = func_data.get('name', '')
            if func_name not in local_namespace:
                print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_name} æœªå®šä¹‰")
                return False, None
            
            func_obj = local_namespace[func_name]
            
            # è®¾ç½®è¶…æ—¶æœºåˆ¶ï¼ˆä»…åœ¨æ”¯æŒçš„ç³»ç»Ÿä¸Šï¼‰
            timeout_set = False
            try:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30ç§’è¶…æ—¶
                timeout_set = True
            except AttributeError:
                # Windowsç³»ç»Ÿä¸æ”¯æŒSIGALRM
                import time
                start_time = time.time()
            
            try:
                # æ‰§è¡Œå‡½æ•°
                result_df = func_obj(df.copy())
                
                # æ•°æ®ç±»å‹ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹å…¼å®¹æ€§
                for col in result_df.columns:
                    if col not in df.columns:  # åªå¤„ç†æ–°ç”Ÿæˆçš„åˆ—
                        try:
                            # å°è¯•ä¿®å¤å¸¸è§çš„æ•°æ®ç±»å‹ä¸å…¼å®¹é—®é¢˜
                            if result_df[col].dtype == 'object':
                                # å°è¯•å°†objectç±»å‹çš„åˆ—è½¬æ¢ä¸ºæ›´å…·ä½“çš„ç±»å‹ï¼Œä½†ä¿æŒå…¼å®¹æ€§
                                temp_series = result_df[col]
                                # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
                                if temp_series.apply(lambda x: pd.isna(x) or isinstance(x, (int, float, str, pd.Timestamp))).all():
                                    pass  # æ•°æ®ç±»å‹å·²å…¼å®¹
                                else:
                                    # å¦‚æœæœ‰æ··åˆç±»å‹ï¼Œè½¬æ¢ä¸ºobjectç±»å‹
                                    result_df[col] = result_df[col].astype('object')
                        except Exception:
                            # å¦‚æœç±»å‹è½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸæ ·
                            pass
                
                # ä¿æŒåŸå§‹æ—¥æœŸåˆ—çš„æ ¼å¼ä¸€è‡´ï¼ˆå°†è½¬æ¢åçš„æ—¥æœŸåˆ—æ¢å¤ä¸ºåŸå§‹æ ¼å¼ï¼‰
                for col in df.columns:
                    if df[col].dtype == 'object':  # åŸå§‹ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                        # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ï¼ˆYYYY-MM-DDï¼‰
                        sample_vals = df[col].dropna().head(5)
                        if len(sample_vals) > 0:
                            is_date_str = all(isinstance(val, str) and 
                                            len(val) == 10 and 
                                            val.count('-') == 2 and
                                            val.replace('-', '').isdigit() 
                                            for val in sample_vals if pd.notna(val))
                            if is_date_str and col in result_df.columns and pd.api.types.is_datetime64_any_dtype(result_df[col]):
                                # å°†æ—¥æœŸæ—¶é—´åˆ—è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
                                result_df[col] = result_df[col].dt.strftime('%Y-%m-%d').replace('NaT', None)
                
                # å–æ¶ˆè¶…æ—¶ï¼ˆå¦‚æœè®¾ç½®äº†çš„è¯ï¼‰
                if timeout_set:
                    signal.alarm(0)
                else:
                    # Windowsç³»ç»Ÿæ£€æŸ¥æ‰§è¡Œæ—¶é—´
                    elapsed_time = time.time() - start_time
                    if elapsed_time > 30:
                        print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_name} æ‰§è¡Œè¶…æ—¶")
                        return False, None
                
                # éªŒè¯ç»“æœ
                if not isinstance(result_df, pd.DataFrame):
                    print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_name} æœªè¿”å›DataFrame")
                    return False, None
                
                if len(result_df) != len(df):
                    print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_name} è¿”å›çš„è¡Œæ•°ä¸åŒ¹é…")
                    return False, None
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†é¢„æœŸçš„æ–°åˆ—
                expected_new_cols = func_data.get('new_columns', [col_name])
                for new_col in expected_new_cols:
                    if new_col not in result_df.columns:
                        print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_name} æœªç”Ÿæˆé¢„æœŸåˆ— {new_col}")
                        return False, None
                
                print(f"       éªŒè¯æˆåŠŸ: å‡½æ•° {func_name} æ‰§è¡Œå¹¶éªŒè¯é€šè¿‡")
                return True, result_df
                
            except TimeoutError:
                print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_name} æ‰§è¡Œè¶…æ—¶")
                return False, None
            except Exception as e:
                # å–æ¶ˆè¶…æ—¶ï¼ˆå¦‚æœè®¾ç½®äº†çš„è¯ï¼‰
                if timeout_set:
                    signal.alarm(0)
                raise e
            
        except KeyError as e:
            print(f"       éªŒè¯å¤±è´¥: å‡½æ•° {func_data.get('name', 'unknown')} ç¼ºå°‘ä¾èµ–åˆ—: {e}")
            # å¯¹äºKeyErrorï¼Œè®°å½•ç¼ºå¤±çš„ä¾èµ–é¡¹ï¼Œä½†ä¸å®Œå…¨å¤±è´¥
            missing_column = str(e).strip("'\"")
            print(f"       æ£€æµ‹åˆ°ç¼ºå¤±åˆ—: {missing_column}ï¼Œåœ¨å®é™…æ‰§è¡Œæ—¶å¯èƒ½ç”±å…¶ä»–å‡½æ•°ç”Ÿæˆ")
            return False, None
        except Exception as e:
            print(f"       éªŒè¯å¤±è´¥: æ‰§è¡Œå‡½æ•° {func_data.get('name', 'unknown')} æ—¶å‡ºé”™: {e}")
            return False, None

    def _record_dependency_error_with_topology(self, ai_service, requirement, col_name, missing_deps, available_columns, topology_info):
        """
        è®°å½•ä¾èµ–é¡ºåºé”™è¯¯åˆ°æ•°æ®åº“ï¼ˆåŒ…å«æ‹“æ‰‘å›¾ä¿¡æ¯ï¼‰
        """
        error_record = {
            "prompt": requirement,
            "error_type": "dependency_order_error",
            "error_detail": {
                "target_column": col_name,
                "missing_dependencies": list(missing_deps),
                "available_columns": list(available_columns),
                "error_message": f"åˆ— {col_name} ä¾èµ–äº {list(missing_deps)}ï¼Œä½†è¿™äº›åˆ—å°šæœªç”Ÿæˆ",
                "topology_info": topology_info  # åŒ…å«æ‹“æ‰‘å›¾ä¿¡æ¯
            },
            "priority": "high",  # é«˜ä¼˜å…ˆçº§
            "logic_tag": "dependency_order",
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self.qwen_db.save_data("error_history", {
            "error_type": "dependency_order_error",
            "content": error_record,
            "is_golden": 0,
            "logic_tag": "dependency_order",
            "score": 0
        })
        
        print(f"     ğŸ“ å·²è®°å½•ä¾èµ–é¡ºåºé”™è¯¯åˆ°æ•°æ®åº“ï¼ˆå«æ‹“æ‰‘ä¿¡æ¯ï¼‰: {col_name}")

    def _record_execution_error_with_state(self, ai_service, requirement, func_data, col_name, intermediate_states):
        """
        è®°å½•æ‰§è¡Œé”™è¯¯ï¼ˆåŒ…å«ä¸­é—´çŠ¶æ€ï¼‰
        """
        error_record = {
            "prompt": requirement,
            "error_type": "execution_error",
            "error_detail": {
                "function_name": func_data.get('name'),
                "target_column": col_name,
                "implementation": func_data.get('implementation', ''),
                "intermediate_states": intermediate_states,  # åŒ…å«ä¸­é—´çŠ¶æ€
                "error_message": "å‡½æ•°æ‰§è¡Œå¤±è´¥"
            },
            "priority": "high",  # é«˜ä¼˜å…ˆçº§
            "logic_tag": "execution_error",
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self.qwen_db.save_data("error_history", {
            "error_type": "execution_error",
            "content": error_record,
            "is_golden": 0,
            "logic_tag": "execution_error",
            "score": 0
        })

    def _record_structure_error(self, ai_service, requirement, func_data, col_name):
        """
        è®°å½•ç»“æ„é”™è¯¯
        """
        error_record = {
            "prompt": requirement,
            "error_type": "structure_error",
            "error_detail": {
                "function_name": func_data.get('name'),
                "target_column": col_name,
                "provided_data": func_data,
                "error_message": "å‡½æ•°ç»“æ„ä¸å®Œæ•´"
            },
            "priority": "low",
            "logic_tag": "structure_error",
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self.qwen_db.save_data("error_history", {
            "error_type": "structure_error",
            "content": error_record,
            "is_golden": 0,
            "logic_tag": "structure_error",
            "score": 0
        })

    def _record_parsing_error(self, ai_service, requirement, response_text, col_name):
        """
        è®°å½•è§£æé”™è¯¯
        """
        error_record = {
            "prompt": requirement,
            "error_type": "parsing_error",
            "error_detail": {
                "target_column": col_name,
                "response_text": response_text,
                "error_message": "æ— æ³•è§£æAIå“åº”ä¸ºJSON"
            },
            "priority": "medium",
            "logic_tag": "parsing_error",
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self.qwen_db.save_data("error_history", {
            "error_type": "parsing_error",
            "content": error_record,
            "is_golden": 0,
            "logic_tag": "parsing_error",
            "score": 0
        })

    def _record_generation_error(self, ai_service, requirement, col_name):
        """
        è®°å½•ç”Ÿæˆé”™è¯¯
        """
        error_record = {
            "prompt": requirement,
            "error_type": "generation_error",
            "error_detail": {
                "target_column": col_name,
                "error_message": "AIå‡½æ•°ç”Ÿæˆå¤±è´¥"
            },
            "priority": "medium",
            "logic_tag": "generation_error",
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self.qwen_db.save_data("error_history", {
            "error_type": "generation_error",
            "content": error_record,
            "is_golden": 0,
            "logic_tag": "generation_error",
            "score": 0
        })

    def process_data_enhanced(self, df: pd.DataFrame, functions: List[Dict[str, Any]], requirement: str, data_context: Dict) -> tuple:
        """
        å¢å¼ºç‰ˆæ•°æ®å¤„ç†æ–¹æ³• - ä½¿ç”¨é—­ç¯è®­ç»ƒæ–¹æ³•
        """
        try:
            print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆé—­ç¯è®­ç»ƒè§£å†³æ–¹æ¡ˆ")
            print("="*60)
            
            # ç¬¬ä¸€é˜¶æ®µï¼šä¾èµ–åˆ†æ
            dependency_analysis = self._analyze_dependencies_phase(data_context, requirement)
            if not dependency_analysis:
                print("âŒ ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                # å›é€€åˆ°ä¼ ç»Ÿå¤„ç†æ–¹æ³•
                return self.process_data(df, functions, requirement), False
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ”¹è¿›çš„ä¾èµ–æ’åºï¼ˆè€ƒè™‘åŸå§‹åˆ—ï¼‰
            sorted_result = self._sort_dependencies_with_original_columns(dependency_analysis, list(df.columns))
            if not sorted_result:
                print("âŒ ç¬¬äºŒé˜¶æ®µå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                # å›é€€åˆ°ä¼ ç»Ÿå¤„ç†æ–¹æ³•
                return self.process_data(df, functions, requirement), False
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šé—­ç¯è®­ç»ƒåŸå­åŒ–é¡ºåºæ‰§è¡Œä¸éªŒè¯
            executed_functions, final_df, intermediate_states = self._closed_loop_execute_and_validate(
                df, requirement, sorted_result, dependency_analysis
            )
            
            if executed_functions:
                print(f"\nâœ… å¢å¼ºç‰ˆé—­ç¯è®­ç»ƒè§£å†³æ–¹æ¡ˆæˆåŠŸå®Œæˆï¼")
                print(f"   - è¯†åˆ«åˆ—æ•°: {len(dependency_analysis.get('dependency_analysis', []))}")
                print(f"   - åŸå§‹åˆ—æ•°: {len(sorted_result['original_columns'])}")
                print(f"   - éœ€ç”Ÿæˆåˆ—æ•°: {len(sorted_result['new_columns'])}")
                print(f"   - æˆåŠŸæ‰§è¡Œå‡½æ•°æ•°: {len(executed_functions)}")
                print(f"   - æœ€ç»ˆDataFrameåˆ—æ•°: {len(final_df.columns)}")
                print(f"   - ä¸­é—´çŠ¶æ€è®°å½•æ•°: {len(intermediate_states)}")
                
                return final_df, True
            else:
                print(f"\nâŒ å¢å¼ºç‰ˆæ‰§è¡Œé˜¶æ®µå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                # å›é€€åˆ°ä¼ ç»Ÿå¤„ç†æ–¹æ³•
                return self.process_data(df, functions, requirement), False
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆå¤„ç†å¤±è´¥: {e}")
            import traceback
            print(traceback.format_exc())
            # å›é€€åˆ°ä¼ ç»Ÿå¤„ç†æ–¹æ³•
            return self.process_data(df, functions, requirement), False


        try:
            # 1. åŠ è½½æ•°æ®
            self.logger.info(f"å¼€å§‹å¤„ç†: {file_path}")
            self.logger.info(f"å¤„ç†éœ€æ±‚: {requirement}")
            
            df = self._load_excel_data(file_path)
            original_df = df.copy()
            
            # åˆå§‹åŒ–è¿­ä»£å˜é‡
            best_processed_df = df.copy()
            best_functions = []
            best_result = {
                "success": False,
                "message": "æœªç”Ÿæˆæœ‰æ•ˆçš„å¤„ç†å‡½æ•°",
                "iteration": 0
            }
            
            # 2. è¿­ä»£æ‰§è¡Œï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            # è®°å½•æ‰€æœ‰ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_files = []
            final_result_iteration = 0
            
            for iteration in range(1, max_iterations + 1):
                self.logger.info(f"\n=== å¼€å§‹ç¬¬ {iteration}/{max_iterations} æ¬¡è¿­ä»£ ===")
                
                try:
                    # 3. ç”Ÿæˆæ•°æ®ä¸Šä¸‹æ–‡ï¼ˆæ¯æ¬¡è¿­ä»£éƒ½é‡æ–°ç”Ÿæˆï¼Œå¯èƒ½ä¼šæœ‰åŠ¨æ€å˜åŒ–ï¼‰
                    data_context = self._generate_data_context(best_processed_df)
                    
                    # 4. ç”Ÿæˆå‡½æ•°
                    self.logger.info(f"ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼šå¼€å§‹ç”Ÿæˆå¤„ç†å‡½æ•°...")
                    
                    # å‡†å¤‡ä¸Šä¸€æ¬¡çš„é”™è¯¯ä¿¡æ¯
                    last_error = ""
                    if iteration > 1:
                        # è·å–ä¸Šä¸€æ¬¡çš„é”™è¯¯ä¿¡æ¯ï¼Œå¦‚æœæ˜¯JSONDecodeErrorï¼Œåˆ™ä½¿ç”¨ç‰¹å®šæç¤º
                        last_error = best_result.get("message", "")
                        
                        # å¦‚æœä¸Šä¸€æ¬¡æ˜¯JSONæ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç‰¹å®šçš„ä¿®å¤æç¤º
                        if "JSON" in last_error or "json" in last_error:
                            last_error = "ä½ ä¸Šä¸€æ¬¡è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œè¯·æ£€æŸ¥æ‹¬å·å¯¹é½å¹¶ç¡®ä¿æ²¡æœ‰å¤šä½™æ–‡å­—ã€‚"
                        elif "å‡½æ•°" in last_error or "è¯­æ³•" in last_error:
                            # å¦‚æœæ˜¯å‡½æ•°è¯­æ³•é”™è¯¯ï¼Œä½¿ç”¨æ›´å…·ä½“çš„æç¤º
                            last_error = "ä½ ä¸Šä¸€æ¬¡ç”Ÿæˆçš„å‡½æ•°æœ‰è¯­æ³•é”™è¯¯ï¼Œè¯·ç¡®ä¿å‡½æ•°å®šä¹‰æ­£ç¡®ï¼Œä½¿ç”¨defå…³é”®å­—å¼€å¤´ï¼ŒåŒ…å«æ­£ç¡®çš„ç¼©è¿›å’Œè¿”å›è¯­å¥ã€‚"
                        elif "ç¼ºå°‘ä¾èµ–åˆ—" in last_error:
                            # å¦‚æœæ˜¯ç¼ºå°‘ä¾èµ–é¡¹ï¼Œç‰¹åˆ«æé†’AIéœ€è¦å…ˆè®¡ç®—ä¾èµ–é¡¹
                            last_error = f"ä½ ä¸Šä¸€æ¬¡çš„å¤„ç†å¤±è´¥äº†ï¼Œå› ä¸ºç¼ºå°‘å¿…è¦çš„ä¾èµ–åˆ—ã€‚è¯·åˆ†æéœ€æ±‚å¹¶é¦–å…ˆç”Ÿæˆç¼ºå°‘çš„ä¾èµ–åˆ—ï¼Œå¦‚ç¯å¢ƒä¿®æ­£æŒ‡æ•°ã€å®æ—¶å¥åº·å€¼ç­‰ã€‚é”™è¯¯ä¿¡æ¯ï¼š{last_error}ã€‚è¯·æŒ‰æ­¥éª¤åˆ†è§£éœ€æ±‚å¹¶å…ˆè®¡ç®—åŸºç¡€ä¾èµ–é¡¹ã€‚"
                        else:
                            # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œä½¿ç”¨é€šç”¨æç¤º
                            last_error = f"ä½ ä¸Šä¸€æ¬¡ç”Ÿæˆçš„å†…å®¹å­˜åœ¨é—®é¢˜ï¼š{last_error}ï¼Œè¯·é‡æ–°ç”Ÿæˆæœ‰æ•ˆçš„å‡½æ•°ã€‚"
                    
                    functions = self.generate_multi_column_functions(requirement, data_context, last_error)
                    
                    if not functions:
                        self.logger.warning(f"ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼šæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¤„ç†å‡½æ•°ï¼Œå°è¯•ä¸‹ä¸€æ¬¡è¿­ä»£")
                        continue
                    
                    # 5. ä½¿ç”¨å¢å¼ºç‰ˆçš„é—­ç¯è®­ç»ƒæ–¹æ³•å¤„ç†æ•°æ®
                    self.logger.info(f"ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼šå¼€å§‹åº”ç”¨å‡½æ•°å¤„ç†æ•°æ®...")
                    
                    # ä½¿ç”¨å¢å¼ºç‰ˆçš„é—­ç¯è®­ç»ƒæ–¹æ³•
                    processed_df, execution_success = self.process_data_enhanced(best_processed_df, functions, requirement, data_context)
                    
                    # 6. æ£€æŸ¥å¤„ç†ç»“æœ
                    new_columns = list(set(processed_df.columns) - set(original_df.columns))
                    if new_columns or execution_success:
                        self.logger.info(f"ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼šå¤„ç†æˆåŠŸï¼Œæ–°å¢äº† {len(new_columns)} åˆ—")
                        
                        # æ›´æ–°æœ€ä½³ç»“æœ
                        best_processed_df = processed_df.copy()
                        best_functions = functions
                        
                        # æ„å»ºå½“å‰è¿­ä»£çš„æˆåŠŸç»“æœ
                        current_result = {
                            "success": True,
                            "iteration": iteration,
                            "generated_functions": len(functions),
                            "new_columns": new_columns,
                            "original_columns": list(original_df.columns),
                            "processed_columns": list(processed_df.columns),
                            "rows_processed": len(processed_df)
                        }
                        
                        # ä¿å­˜å½“å‰ç»“æœ
                        output_file = os.path.splitext(file_path)[0] + f"_processed_iteration_{iteration}.xlsx"
                        processed_df.to_excel(output_file, index=False)
                        temp_files.append(output_file)  # è®°å½•ä¸´æ—¶æ–‡ä»¶
                        current_result["output_path"] = output_file
                        current_result["message"] = f"ç¬¬ {iteration} æ¬¡è¿­ä»£å¤„ç†æˆåŠŸ"
                        
                        # æ›´æ–°æœ€ä½³ç»“æœ
                        best_result = current_result
                        final_result_iteration = iteration  # è®°å½•æœ€ç»ˆç»“æœçš„è¿­ä»£æ¬¡æ•°
                        
                        # å¦‚æœç”Ÿæˆäº†æœŸæœ›çš„åˆ—ï¼Œå¯ä»¥æå‰ç»“æŸè¿­ä»£
                        if len(new_columns) >= 1:  # å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ç»ˆæ­¢æ¡ä»¶
                            self.logger.info(f"ç¬¬ {iteration} æ¬¡è¿­ä»£å·²ç”Ÿæˆæœ‰æ•ˆç»“æœï¼Œæå‰ç»“æŸè¿­ä»£")
                            break
                    else:
                        self.logger.warning(f"ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼šæœªç”Ÿæˆæ–°åˆ—ï¼Œå°è¯•ä¸‹ä¸€æ¬¡è¿­ä»£")
                        
                except json.JSONDecodeError:
                    self.logger.error(f"ç¬¬ {iteration} æ¬¡è¿­ä»£å¤±è´¥: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
                    # è®°å½•é”™è¯¯å¹¶å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£çš„æç¤º
                    self._get_ai_service().add_error({
                        "prompt": requirement,
                        "error": "ä½ ä¸Šä¸€æ¬¡è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œè¯·æ£€æŸ¥æ‹¬å·å¯¹é½å¹¶ç¡®ä¿æ²¡æœ‰å¤šä½™æ–‡å­—ã€‚",
                        "iteration": iteration,
                        "traceback": "JSONDecodeError"
                    })
                    
                except KeyError as e:
                    # ç‰¹åˆ«å¤„ç†ç¼ºå°‘ä¾èµ–åˆ—çš„é”™è¯¯
                    missing_column = str(e)
                    self.logger.error(f"ç¬¬ {iteration} æ¬¡è¿­ä»£å¤±è´¥: ç¼ºå°‘ä¾èµ–åˆ— {missing_column}")
                    
                    # å°†ç¼ºå°‘ä¾èµ–åˆ—çš„ä¿¡æ¯åé¦ˆç»™AIæœåŠ¡ï¼Œè¦æ±‚AIå…ˆè®¡ç®—ä¾èµ–é¡¹
                    self._get_ai_service().add_error({
                        "prompt": requirement,
                        "error": f"ç¼ºå°‘ä¾èµ–åˆ—: {missing_column}ã€‚è¯·æŒ‰éœ€æ±‚åˆ†è§£æ­¥éª¤ï¼Œå…ˆè®¡ç®—åŸºç¡€ä¾èµ–åˆ—ã€‚",
                        "iteration": iteration,
                        "traceback": f"KeyError: {missing_column}"
                    })
                    
                except Exception as e:
                    self.logger.error(f"ç¬¬ {iteration} æ¬¡è¿­ä»£å¤±è´¥: {e}")
                    import traceback
                    self.logger.error(traceback.format_exc())
                    
                    # å°†é”™è¯¯ä¿¡æ¯åé¦ˆç»™AIæœåŠ¡ï¼Œç”¨äºåç»­è¿­ä»£ä¼˜åŒ–
                    self._get_ai_service().add_error({
                        "prompt": requirement,
                        "error": str(e),
                        "iteration": iteration,
                        "traceback": traceback.format_exc()
                    })
            
            # æœ€ç»ˆä¿å­˜æœ€ä½³ç»“æœ
            if best_result["success"]:
                final_output_file = os.path.splitext(file_path)[0] + "_processed.xlsx"
                best_processed_df.to_excel(final_output_file, index=False)
                best_result["final_output_path"] = final_output_file
                best_result["message"] += f"ï¼Œæœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {final_output_file}"
                
                # æ¸…ç†æ‰€æœ‰è¿­ä»£è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ–‡ä»¶
                for temp_file in temp_files:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                
                return best_result
            else:
                # æ‰€æœ‰è¿­ä»£éƒ½å¤±è´¥
                return {
                    "success": False,
                    "error": "æ‰€æœ‰è¿­ä»£å‡å¤±è´¥",
                    "message": f"ç»è¿‡ {max_iterations} æ¬¡è¿­ä»£ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¤„ç†ç»“æœ",
                    "max_iterations": max_iterations
                }
        except Exception as e:
            self.logger.error(f"å¤„ç†å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e),
                "message": "å¤„ç†å¤±è´¥"
            }
    
    def batch_process(self, file_paths: List[str], requirement: str) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªExcelæ–‡ä»¶
    
        Args:
            file_paths: Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            requirement: ç”¨æˆ·éœ€æ±‚
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        for file_path in file_paths:
            result = self.process_multi_columns(file_path, requirement)
            result["file_path"] = file_path
            results.append(result)
        return results

# åˆ›å»ºå…¨å±€å¤šåˆ—å¤„ç†å™¨å®ä¾‹
multi_column_processor = MultiColumnProcessor()